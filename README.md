
# ğŸ“š RAG PDF Chatbot

This project is a **Retrieval-Augmented Generation (RAG) PDF Chatbot** built using **Streamlit, LangChain, FAISS, Pinecone, HuggingFace, and Cohere/Groq/OpenAI**.  
It allows users to upload PDFs, process them into **chunks**, create **embeddings**, store them in a vector database (**FAISS/Pinecone**), and then query them using an **LLM** for intelligent answers.

---

Live App: [https://groqragapp-cpep2ffsqzojavjlvicy5d.streamlit.app/](https://groqragapp-cpep2ffsqzojavjlvicy5d.streamlit.app/)

## ğŸš€ Features
- ğŸ“¤ Upload PDFs directly in the web app
- ğŸ“‘ Extract and clean text from PDFs (supports scanned PDFs via OCR)
- âœ‚ï¸ Chunking of text into manageable pieces
- ğŸ§© Embedding generation using **Sentence Transformers / Cohere / OpenAI**
- ğŸ—‚ï¸ Store and search chunks in **FAISS or Pinecone**
- ğŸ’¬ Ask questions and get answers powered by **LLMs (Groq / OpenAI / Cohere)**
- ğŸ“Š **Frontend pipeline visualization** showing:
  - âœ… File uploaded  
  - âš™ï¸ PDF loaded  
  - âœ‚ï¸ Chunking  
  - ğŸ§© Embedding  
  - ğŸ’¾ Storing in Vector DB  
  - ğŸ¤– Ready to Chat!  

---

## ğŸ› ï¸ Tech Stack
- [Streamlit](https://streamlit.io/) â€“ UI framework
- [LangChain](https://www.langchain.com/) â€“ RAG pipeline
- [FAISS](https://github.com/facebookresearch/faiss) / [Pinecone](https://www.pinecone.io/) â€“ Vector DB
- [Cohere](https://cohere.com/),[Sentence-Transformers](https://www.sbert.net/) â€“ Embedding models
-  [Groq](https://groq.com/), [OpenAI](https://openai.com/) â€“ LLM providers
- [PyPDF](https://pypi.org/project/pypdf/) / [Unstructured](https://unstructured-io.github.io/) â€“ PDF parsing

---

## ğŸ“‚ Project Structure
```
.
â”œ
â”‚-- multidoc.py (Main Python file)
|--requirements.txt
|--.env.examples
|--Readme.md
```

---

## âš™ï¸ Installation

1. **Clone repo**
```bash
git clone https://github.com/Nitinpratap22061/groqragapp.git
cd rag-pdf-chatbot
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Setup environment variables**  
Create a `.env` file in the root directory:
```ini
OPENAI_API_KEY=your_openai_api_key
COHERE_API_KEY=your_cohere_api_key
GROQ_API_KEY=your_groq_api_key
PINECONE_API_KEY=your_pinecone_api_key
```

---

## â–¶ï¸ Run the App
```bash
streamlit run frontend/app.py
```

---

## ğŸ”„ RAG Pipeline (Stepwise)
1. **ğŸ“¤ Upload PDF**
2. **ğŸ“‘ Extract text**
3. **âœ‚ï¸ Chunking**
4. **ğŸ§© Generate embeddings**
5. **ğŸ’¾ Store in FAISS/Pinecone**
6. **ğŸ¤– Query LLM for answers**

---

## ğŸ“¸ Example Frontend Visualization

âœ… File uploaded â†’ âš™ï¸ PDF Loaded â†’ âœ‚ï¸ Chunking â†’ ğŸ§© Embedding â†’ ğŸ’¾ Stored â†’ ğŸ¤– Ready to Chat!  

---

## ğŸ“ Short Note on RAG Answer Evaluation

We tested a Retrieval-Augmented Generation (RAG) system using a knowledge base PDF on Artificial Intelligence basics. Five descriptive questions were asked, and the systemâ€™s answers were compared with the actual content of the document.

**Precision:** The proportion of retrieved/generated answers that were correct.  
**Recall:** The proportion of correct answers from the knowledge base that were successfully retrieved by RAG.

**Comparison of RAG vs. Actual Answers:**

- **Q1 (Definition of AI):** RAG answer matched perfectly with knowledge base â†’ âœ… Correct.  
- **Q2 (Applications of AI):** RAG correctly retrieved examples (Siri, Alexa, Netflix) â†’ âœ… Correct.  
- **Q3 (Definition of ML):** RAG answer matched knowledge base definition â†’ âœ… Correct.  
- **Q4 (Deep Learning):** RAG failed to retrieve from the PDF â†’ âŒ Missed (Recall issue).  
- **Q5 (AI in healthcare & education):** RAG partially correct (healthcare/education examples were not in PDF, but valid generally) â†’ âŒ Missed (strict evaluation).

**Performance:**

- **Precision:** 3/5 = 0.6 (60%)  
- **Recall:** 3/4 = 0.75 (75%)

**Observation:**  
RAG was able to give correct answers for definitions and direct examples but struggled when information was either missing (Q4) or outside the given knowledge base (Q5). This shows RAG can generate plausible but not always knowledge-base-grounded answers.

---

## ğŸ¤ Contributing
Feel free to fork this repo, raise issues, and submit PRs.

---

## ğŸ“œ License
MIT License Â© 2025

---

## ğŸš€ Demo

Live App: [https://groqragapp.onrender.com/](https://groqragapp-cpep2ffsqzojavjlvicy5d.streamlit.app/)
